#!/bin/bash
# Job name
#PBS -N H2
# Output files
#PBS -o ./H2.o
#PBS -e ./H2.e
# Queue name
#PBS -q short_cpuQ
# Set the maximum wall time
#PBS -l walltime=0:02:00
# Number of nodes, cpus, mpi processors and amount of memory
#PBS -l select=1:ncpus=96:mpiprocs=96:mem=1gb

# Modules for python and MPI
module load gcc91
module load mpich-3.2.1--gcc-9.1.0

gcc() {
    gcc-9.1.0 "$@"
}
gcc --version


# Print the name of the file that contains the list of the nodes assigned to the job and list all the nodes
NODES=$(cat $PBS_NODEFILE)
echo The running nodes are $NODES

# Get the list of unique nodes assigned to the job
NODES=$(sort -u $PBS_NODEFILE)
echo The running nodes are $NODES

# Loop through each node and get architecture information
for NODE in $NODES; do
    echo "Node: $NODE"
    ssh $NODE "lscpu"
done

# Select the working directory
cd /home/mose.arcaro/H2

# the code should be previously compiled
mpicc code_mpi.c -o code.out

# Run the code
mpirun -np 4 ./code.out
# If you set the number of mpi processors, here it is enough to type
# mpirun ./code.out