#!/bin/bash
# Job name
#PBS -N partial1
# Output files
#PBS -o ./p1.o
#PBS -e ./p1.e
# Queue name
#PBS -q short_cpuQ
# Set the maximum wall time
#PBS -l walltime=01:00:00
# Number of nodes, cpus, mpi processors and amount of memory
#PBS -l select=1:ncpus=96:mpiprocs=96:ompthreads=96:mem=1gb

# Modules for python and MPI
module load gcc91
module load mpich-3.2.1--gcc-9.1.0
module load python-3.7.2

gcc() {
    gcc-9.1.0 "$@"
}
gcc --version


# Print the name of the file that contains the list of the nodes assigned to the job and list all the nodes
NODES=$(cat $PBS_NODEFILE)
echo The running nodes are $NODES

# Get the list of unique nodes assigned to the job
NODES=$(sort -u $PBS_NODEFILE)
echo The running nodes are $NODES

# Loop through each node and get architecture information
for NODE in $NODES; do
    echo "Node: $NODE"
    ssh $NODE "lscpu"
done

# Select the working directory
cd /home/mose.arcaro/H2/BaseVsAlltoallVsDatatype

# the code should be previously compiled
#mpicc code_mpi.c -o code.out

# Run the code
#mpirun -np 4 ./code.out
# If you set the number of mpi processors, here it is enough to type
# mpirun ./code.out

#find the best mpi version vs seq (table)
touch output.csv
echo "Processes,Dim,CheckSym,MatTranspose,Version" > output.csv

for (( k = 16; k <= 4096; k=k*2 )); do

  gcc -o SEQ SEQ.c

    ./SEQ "$k"


  mpicc MPI_Base.c -o MPI_Base
  for (( m = 1; m <= k && m <= 96; m++ )); do ########!

      mpirun -np "$m" ./MPI_Base "$k" "$m"


  done

  mpicc MPI_All.c -o MPI_All
  if (($k < 128)); then

      mpirun -np "$k" ./MPI_All "$k" "$k"

  fi

  mpicc MPI_DataT.c -o MPI_DataT
  for (( m = 1; m <= k && m <= 96; m++ )); do

      mpirun -np "$m" ./MPI_DataT "$k" "$m"


  done

done