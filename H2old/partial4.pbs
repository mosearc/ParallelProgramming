#!/bin/bash
# Job name
#PBS -N partial4
# Output files
#PBS -o ./p4.o
#PBS -e ./p4.e
# Queue name
#PBS -q short_cpuQ
# Set the maximum wall time
#PBS -l walltime=01:00:00
# Number of nodes, cpus, mpi processors and amount of memory
#PBS -l select=1:ncpus=96:mpiprocs=96:ompthreads=96:mem=1gb

# Modules for python and MPI
module load gcc91
module load mpich-3.2.1--gcc-9.1.0
module load python-3.7.2

gcc() {
    gcc-9.1.0 "$@"
}
gcc --version


# Print the name of the file that contains the list of the nodes assigned to the job and list all the nodes
NODES=$(cat $PBS_NODEFILE)
echo The running nodes are $NODES

# Get the list of unique nodes assigned to the job
NODES=$(sort -u $PBS_NODEFILE)
echo The running nodes are $NODES

# Loop through each node and get architecture information
for NODE in $NODES; do
    echo "Node: $NODE"
    ssh $NODE "lscpu"
done

# Select the working directory
cd /home/mose.arcaro/H2/
cd mpiBlock
#comparison no blocking vs blocking (graph)
touch outputBvNB.csv
echo "Processes,Dim,CheckSym,MatTranspose,Version" > outputBvNB.csv

for (( k = 16; k <= 4096; k=k*2 )); do

  gcc -o SEQ SEQ.c

    ./SEQ "$k"


  mpicc MPIB_DataTV.c -o MPIB_DataTV -lm
  for (( m = 1; m <= k && m <= 96; m++ )); do

      mpirun -np "$m" ./MPIB_DataTV "$k" "$m"


  done

  mpicc MPI_DataT.c -o MPI_DataT
  for (( m = 1; m <= k && m <= 96; m++ )); do

      mpirun -np "$m" ./MPI_DataT "$k" "$m"


  done

done